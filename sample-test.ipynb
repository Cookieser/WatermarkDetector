{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import sys\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\")\n",
    "sys.path.append('src')\n",
    "from omegaconf import OmegaConf \n",
    "from diffusers import StableDiffusionPipeline \n",
    "from utils_model import load_model_from_config \n",
    "import random\n",
    "import json\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "ldm_config = \"ckpt/v2-inference.yaml\"\n",
    "ldm_ckpt = \"ckpt/v2-1_512-ema-pruned.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Total images selected: 16\n",
      "Example captions:\n",
      "1: A large blue clock tower above an old brick building.\n",
      "\n",
      "2: Large group of stuffed animals sitting on top of a red bed. \n",
      "3: People swimming in the ocean, one is surfing.\n",
      "4: a woman is sitting up in her bed \n",
      "5: A large truck made for hauling loads is stopped at a stop sign.\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "\n",
    "ann_path = 'coco/annotations/captions_train2014.json'\n",
    "coco = COCO(ann_path)\n",
    "\n",
    "img_ids = coco.getImgIds()\n",
    "random.seed(42)\n",
    "random.shuffle(img_ids)\n",
    "\n",
    "\n",
    "captions = []\n",
    "for img_id in img_ids[:16]:  \n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    if anns:\n",
    "        captions.append(anns[0]['caption'])  \n",
    "\n",
    "print(f\"Total images selected: {len(captions)}\")\n",
    "print(\"Example captions:\")\n",
    "for i, cap in enumerate(captions[:5]):\n",
    "    print(f\"{i+1}: {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Building LDM model with config ckpt/v2-inference.yaml and weights from ckpt/v2-1_512-ema-pruned.ckpt...\n",
      "Loading model from ckpt/v2-1_512-ema-pruned.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yw699/codes_lab0/stable_signature/utils_model.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 220000\n",
      "No module 'xformers'. Proceeding without it.\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2415765/20358034.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"ckpt/sd2_decoder.pth\")\n",
      "/home/yw699/anaconda3/envs/forgery-watermark/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['encoder.conv_in.weight', 'encoder.conv_in.bias', 'encoder.down.0.block.0.norm1.weight', 'encoder.down.0.block.0.norm1.bias', 'encoder.down.0.block.0.conv1.weight', 'encoder.down.0.block.0.conv1.bias', 'encoder.down.0.block.0.norm2.weight', 'encoder.down.0.block.0.norm2.bias', 'encoder.down.0.block.0.conv2.weight', 'encoder.down.0.block.0.conv2.bias', 'encoder.down.0.block.1.norm1.weight', 'encoder.down.0.block.1.norm1.bias', 'encoder.down.0.block.1.conv1.weight', 'encoder.down.0.block.1.conv1.bias', 'encoder.down.0.block.1.norm2.weight', 'encoder.down.0.block.1.norm2.bias', 'encoder.down.0.block.1.conv2.weight', 'encoder.down.0.block.1.conv2.bias', 'encoder.down.0.downsample.conv.weight', 'encoder.down.0.downsample.conv.bias', 'encoder.down.1.block.0.norm1.weight', 'encoder.down.1.block.0.norm1.bias', 'encoder.down.1.block.0.conv1.weight', 'encoder.down.1.block.0.conv1.bias', 'encoder.down.1.block.0.norm2.weight', 'encoder.down.1.block.0.norm2.bias', 'encoder.down.1.block.0.conv2.weight', 'encoder.down.1.block.0.conv2.bias', 'encoder.down.1.block.0.nin_shortcut.weight', 'encoder.down.1.block.0.nin_shortcut.bias', 'encoder.down.1.block.1.norm1.weight', 'encoder.down.1.block.1.norm1.bias', 'encoder.down.1.block.1.conv1.weight', 'encoder.down.1.block.1.conv1.bias', 'encoder.down.1.block.1.norm2.weight', 'encoder.down.1.block.1.norm2.bias', 'encoder.down.1.block.1.conv2.weight', 'encoder.down.1.block.1.conv2.bias', 'encoder.down.1.downsample.conv.weight', 'encoder.down.1.downsample.conv.bias', 'encoder.down.2.block.0.norm1.weight', 'encoder.down.2.block.0.norm1.bias', 'encoder.down.2.block.0.conv1.weight', 'encoder.down.2.block.0.conv1.bias', 'encoder.down.2.block.0.norm2.weight', 'encoder.down.2.block.0.norm2.bias', 'encoder.down.2.block.0.conv2.weight', 'encoder.down.2.block.0.conv2.bias', 'encoder.down.2.block.0.nin_shortcut.weight', 'encoder.down.2.block.0.nin_shortcut.bias', 'encoder.down.2.block.1.norm1.weight', 'encoder.down.2.block.1.norm1.bias', 'encoder.down.2.block.1.conv1.weight', 'encoder.down.2.block.1.conv1.bias', 'encoder.down.2.block.1.norm2.weight', 'encoder.down.2.block.1.norm2.bias', 'encoder.down.2.block.1.conv2.weight', 'encoder.down.2.block.1.conv2.bias', 'encoder.down.2.downsample.conv.weight', 'encoder.down.2.downsample.conv.bias', 'encoder.down.3.block.0.norm1.weight', 'encoder.down.3.block.0.norm1.bias', 'encoder.down.3.block.0.conv1.weight', 'encoder.down.3.block.0.conv1.bias', 'encoder.down.3.block.0.norm2.weight', 'encoder.down.3.block.0.norm2.bias', 'encoder.down.3.block.0.conv2.weight', 'encoder.down.3.block.0.conv2.bias', 'encoder.down.3.block.1.norm1.weight', 'encoder.down.3.block.1.norm1.bias', 'encoder.down.3.block.1.conv1.weight', 'encoder.down.3.block.1.conv1.bias', 'encoder.down.3.block.1.norm2.weight', 'encoder.down.3.block.1.norm2.bias', 'encoder.down.3.block.1.conv2.weight', 'encoder.down.3.block.1.conv2.bias', 'encoder.mid.block_1.norm1.weight', 'encoder.mid.block_1.norm1.bias', 'encoder.mid.block_1.conv1.weight', 'encoder.mid.block_1.conv1.bias', 'encoder.mid.block_1.norm2.weight', 'encoder.mid.block_1.norm2.bias', 'encoder.mid.block_1.conv2.weight', 'encoder.mid.block_1.conv2.bias', 'encoder.mid.attn_1.norm.weight', 'encoder.mid.attn_1.norm.bias', 'encoder.mid.attn_1.q.weight', 'encoder.mid.attn_1.q.bias', 'encoder.mid.attn_1.k.weight', 'encoder.mid.attn_1.k.bias', 'encoder.mid.attn_1.v.weight', 'encoder.mid.attn_1.v.bias', 'encoder.mid.attn_1.proj_out.weight', 'encoder.mid.attn_1.proj_out.bias', 'encoder.mid.block_2.norm1.weight', 'encoder.mid.block_2.norm1.bias', 'encoder.mid.block_2.conv1.weight', 'encoder.mid.block_2.conv1.bias', 'encoder.mid.block_2.norm2.weight', 'encoder.mid.block_2.norm2.bias', 'encoder.mid.block_2.conv2.weight', 'encoder.mid.block_2.conv2.bias', 'encoder.norm_out.weight', 'encoder.norm_out.bias', 'encoder.conv_out.weight', 'encoder.conv_out.bias', 'quant_conv.weight', 'quant_conv.bias'], unexpected_keys=[])\n",
      "you should check that the decoder keys are correctly matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  17%|█▋        | 1/6 [00:05<00:27,  5.54s/it]/home/yw699/anaconda3/envs/forgery-watermark/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/yw699/anaconda3/envs/forgery-watermark/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:08<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f'>>> Building LDM model with config {ldm_config} and weights from {ldm_ckpt}...')\n",
    "config = OmegaConf.load(f\"{ldm_config}\")\n",
    "ldm_ae = load_model_from_config(config, ldm_ckpt)\n",
    "ldm_aef = ldm_ae.first_stage_model\n",
    "ldm_aef.eval()\n",
    "\n",
    "# loading the fine-tuned decoder weights\n",
    "state_dict = torch.load(\"ckpt/sd2_decoder.pth\")\n",
    "unexpected_keys = ldm_aef.load_state_dict(state_dict, strict=False)\n",
    "print(unexpected_keys)\n",
    "print(\"you should check that the decoder keys are correctly matched\")\n",
    "\n",
    "# loading the pipeline, and replacing the decode function of the pipe\n",
    "model = \"stabilityai/stable-diffusion-2\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"the cat drinks water.\"\n",
    "\n",
    "# generator = torch.manual_seed(seed)\n",
    "# img_orig = pipe(prompt,generator = generator).images[0]\n",
    "# img_orig.save(\"cat_original.png\")\n",
    "\n",
    "# pipe.vae.decode = (lambda x,  *args, **kwargs: ldm_aef.decode(x).unsqueeze(0))\n",
    "\n",
    "\n",
    "\n",
    "# generator = torch.manual_seed(seed)\n",
    "# img = pipe(prompt,generator = generator).images[0]\n",
    "# img.save(\"cat_watermarked.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = torch.manual_seed(seed)\n",
    "# prompts = [ \"a pig\",\"a dragon\"]\n",
    "# results = pipe(prompts, generator=generator).images\n",
    "\n",
    "# for i, img in enumerate(results):\n",
    "#     img.save(f\"output_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"test\"\n",
    "os.makedirs(f\"{output_dir}/original\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/watermarked\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.62s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.66s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.66s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n",
      "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m         dataset_records\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: idx \u001b[38;5;241m+\u001b[39m i,\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_prompts[i],\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m: orig_paths[i],\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermarked\u001b[39m\u001b[38;5;124m\"\u001b[39m: wm_path\n\u001b[1;32m     31\u001b[0m         })\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/metadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(dataset_records, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Done! Total samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataset_records = []\n",
    "original_decode = pipe.vae.decode\n",
    "for idx in range(0, len(captions), batch_size):\n",
    "    batch_prompts = captions[idx:idx+batch_size]\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed + idx)\n",
    "    pipe.vae.decode = original_decode  \n",
    "    image_origs = pipe(batch_prompts, generator=generator).images\n",
    "\n",
    "    orig_paths = []\n",
    "    for i, img in enumerate(image_origs):\n",
    "        orig_path = f\"{output_dir}/original/img_{idx + i:04d}.png\"\n",
    "        img.save(orig_path)\n",
    "        orig_paths.append(orig_path)\n",
    "\n",
    "\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed + idx)\n",
    "    pipe.vae.decode = lambda x, *args, **kwargs: ldm_aef.decode(x).unsqueeze(0)\n",
    "    image_watermarked = pipe(batch_prompts, generator=generator).images\n",
    "\n",
    "    for i, img in enumerate(image_origs):\n",
    "        wm_path = f\"{output_dir}/watermarked/img_{idx + i:04d}.png\"\n",
    "        img.save(wm_path)\n",
    "    \n",
    "        dataset_records.append({\n",
    "            \"id\": idx + i,\n",
    "            \"prompt\": batch_prompts[i],\n",
    "            \"original\": orig_paths[i],\n",
    "            \"watermarked\": wm_path\n",
    "        })\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Total samples: 16\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{output_dir}/metadata.json\", \"w\") as f:\n",
    "    json.dump(dataset_records, f, indent=2)\n",
    "\n",
    "print(f\"✅ Done! Total samples: {len(dataset_records)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forgery-watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
